{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24585b71",
   "metadata": {},
   "source": [
    "# Demo of Classification\n",
    "\n",
    "This notebook demonstrates the code necessary to train different classifiers and use these to predict new MSMS spectra of either being of \"interest\" or \"other\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f0dcd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AnnoMe.Classification import (\n",
    "    generate_embeddings,\n",
    "    add_all_metadata,\n",
    "    show_dataset_overview,\n",
    "    generate_embedding_plots,\n",
    "    train_and_classify,\n",
    "    generate_prediction_overview,\n",
    "    generate_ml_metrics_overview,\n",
    "    set_random_seeds,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "\n",
    "set_random_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa309e",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "# parameters\n",
    "\n",
    "# Output directory\n",
    "output_dir = f\"./output/PrenylatedCompounds_BOKUDB/\"\n",
    "\n",
    "# Main datasets to process for classification\n",
    "base_folder = \"../resources/libraries_other\"\n",
    "datasets = [\n",
    "    # MS/MS of reference prenylated flavones\n",
    "    {\n",
    "        \"name\": \"prenylated flavonoids - gt relevant\",\n",
    "        \"type\": \"train - relevant\",\n",
    "        \"file\": os.path.join(base_folder, \"HCD_pos__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#D41F11\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"prenylated flavonoids - gt relevant\",\n",
    "        \"type\": \"train - relevant\",\n",
    "        \"file\": os.path.join(base_folder, \"HCD_neg__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#D41F11\",\n",
    "    },\n",
    "\n",
    "    # MS/MS of wheat samples\n",
    "    {\n",
    "        \"name\": \"wheat metabolites - gt other\",\n",
    "        \"type\": \"train - other\",\n",
    "        \"file\": os.path.join(base_folder, \"Wheat_pos__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#017192\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"wheat metabolites - gt other\",\n",
    "        \"type\": \"train - other\",\n",
    "        \"file\": os.path.join(base_folder, \"Wheat_neg__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#017192\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"wheat metabolites - gt other\",\n",
    "        \"type\": \"train - other\",\n",
    "        \"file\": os.path.join(base_folder, \"n_wheat_pos__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#017192\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"wheat metabolites - gt other\",\n",
    "        \"type\": \"train - other\",\n",
    "        \"file\": os.path.join(base_folder, \"n_wheat_neg__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#017192\",\n",
    "    },\n",
    "\n",
    "    ## MS/MS of BOKU MassBank\n",
    "    {\n",
    "        \"name\": \"MassBank BOKU - gt relevant\",\n",
    "        \"type\": \"train - relevant\",\n",
    "        \"file\": os.path.join(\"..\", \"resources\", \"libraries_filtered\", \"BOKU_iBAM___prenyl_flavonoid_or_chalcone__MatchingSmiles.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#80BF02\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MassBank BOKU - gt other\",\n",
    "        \"type\": \"train - other\",\n",
    "        \"file\": os.path.join(\"..\", \"resources\", \"libraries_filtered\", \"BOKU_iBAM___prenyl_flavonoid_or_chalcone__NonMatchingSmiles.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#80BF02\",\n",
    "    },\n",
    "\n",
    "    # MS/MS for inference\n",
    "    {\n",
    "        \"name\": \"Paulowina tomentosa\",\n",
    "        \"type\": \"inference\",\n",
    "        \"file\": os.path.join(base_folder, \"n_PT22_neg__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#8B0773\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Paulowina tomentosa\",\n",
    "        \"type\": \"inference\",\n",
    "        \"file\": os.path.join(base_folder, \"n_PT22_pos__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#8B0773\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Paulowina tomentosa\",\n",
    "        \"type\": \"inference\",\n",
    "        \"file\": os.path.join(base_folder, \"n_PT24_neg__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#8B0773\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Paulowina tomentosa\",\n",
    "        \"type\": \"inference\",\n",
    "        \"file\": os.path.join(base_folder, \"n_PT24_pos__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#8B0773\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Paulowina tomentosa\",\n",
    "        \"type\": \"inference\",\n",
    "        \"file\": os.path.join(base_folder, \"PT22CH_neg__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#8B0773\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Paulowina tomentosa\",\n",
    "        \"type\": \"inference\",\n",
    "        \"file\": os.path.join(base_folder, \"PT22CH_pos__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#8B0773\",\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Glycyrrhizza uralensis\",\n",
    "        \"type\": \"inference\",\n",
    "        \"file\": os.path.join(base_folder, \"n_GU_neg__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#8B0773\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Glycyrrhizza uralensis\",\n",
    "        \"type\": \"inference\",\n",
    "        \"file\": os.path.join(base_folder, \"n_GU_pos__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#8B0773\",\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Samp1\",\n",
    "        \"type\": \"inference\",\n",
    "        \"file\": os.path.join(base_folder, \"Samp1_neg__sirius.mgf\"),\n",
    "        \"fragmentation_method\": \"fragmentation_method\",\n",
    "        \"colour\": \"#8B0773\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# meta-data to add to the output from the MS/MS spectra\n",
    "data_to_add = OrderedDict(\n",
    "    [\n",
    "        (\"name\", [\"feature_id\", \"name\", \"title\", \"compound_name\"]),\n",
    "        (\"formula\", [\"formula\"]),\n",
    "        (\"smiles\", [\"smiles\"]),\n",
    "        (\"adduct\", [\"adduct\", \"precursor_type\"]),\n",
    "        (\"ionMode\", [\"ionmode\"]),\n",
    "        (\"RTINSECONDS\", [\"rtinseconds\", \"retention_time\"]),\n",
    "        (\"precursor_mz\", [\"pepmass\", \"precursor_mz\"]),\n",
    "        (\"fragmentation_method\", [\"fragmentation_method\", \"fragmentation_mode\"]),\n",
    "        (\"CE\", [\"collision_energy\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_subsets = {\n",
    "    \n",
    "    \"hcd_neg\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"negative\"),\n",
    "    \"hcd_pos\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"positive\"),\n",
    "\n",
    "    \"hcd_neg_20.0\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"negative\") & (x[\"CE\"] == \"20.0\"),\n",
    "    \"hcd_neg_30.0\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"negative\") & (x[\"CE\"] == \"30.0\"),\n",
    "    \"hcd_neg_40.0\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"negative\") & (x[\"CE\"] == \"40.0\"),\n",
    "    \"hcd_neg_step[20,45,70]\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"negative\") & (x[\"CE\"] in [\"45.0\", \"stepped20,45,70ev(absolute)\"]),\n",
    "    \n",
    "    \"hcd_pos_20.0\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"positive\") & (x[\"CE\"] == \"20.0\"),\n",
    "    \"hcd_pos_30.0\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"positive\") & (x[\"CE\"] == \"30.0\"),\n",
    "    \"hcd_pos_40.0\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"positive\") & (x[\"CE\"] == \"40.0\"),\n",
    "    \"hcd_pos_step[20,45,70]\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"positive\") & (x[\"CE\"] in [\"45.0\", \"stepped20,45,70ev(absolute)\"]),\n",
    "}\n",
    "\n",
    "# derived, do not change\n",
    "colors = {ds[\"name\"]: ds[\"colour\"] for ds in datasets}\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedeb524",
   "metadata": {},
   "source": [
    "## Execute pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45256b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "pickle_file = f\"{output_dir}/df_embeddings.pkl\"\n",
    "if os.path.exists(pickle_file):\n",
    "    df = pd.read_pickle(pickle_file)\n",
    "    show_dataset_overview(df, print_method=display)\n",
    "\n",
    "else:\n",
    "    # Import the spectra and process MS2DeepScore embeddings\n",
    "    df = generate_embeddings(datasets, data_to_add)\n",
    "\n",
    "    # add associated metadata\n",
    "    df = add_all_metadata(datasets, df)\n",
    "\n",
    "    # export dataframe for re-use\n",
    "    df.to_pickle(f\"{output_dir}/df_embeddings.pkl\")\n",
    "\n",
    "# iterate over the training subsets, produces better output\n",
    "for subset_name in training_subsets:\n",
    "    print(f\"Processing subset: {subset_name}\")\n",
    "    print(f\"##############################################################################\")\n",
    "\n",
    "    # Get the subset function\n",
    "    subset_fn = training_subsets[subset_name]\n",
    "\n",
    "    # show overview and plot\n",
    "    generate_embedding_plots(df, output_dir, colors)\n",
    "\n",
    "    # Create output directory for the subset\n",
    "    c_output_dir = f\"{output_dir}/subset_{subset_name}/\"\n",
    "    os.makedirs(c_output_dir, exist_ok=True)\n",
    "\n",
    "    # subset the dataframe\n",
    "    df_subset = df[df.apply(subset_fn, axis=1)].reset_index(drop=True)\n",
    "\n",
    "    # train and predict new datasets\n",
    "    df_train, df_validation, df_inference, df_metrics = train_and_classify(df_subset, subsets=subset_fn, output_dir=c_output_dir)\n",
    "    generate_prediction_overview(df_subset, df_train, c_output_dir, \"training\", min_prediction_threshold=13)\n",
    "    generate_prediction_overview(df_subset, df_inference, c_output_dir, \"inference\", min_prediction_threshold=13)\n",
    "\n",
    "    # Generate an overview of the machine learning metrics\n",
    "    generate_ml_metrics_overview(df_metrics, c_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an overview of the training datasets obtained from the BOKU repositories\n",
    "print(f\"\\nGenerating an overview of the training datasets obtained from the BOKU repositories\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_training_input = []\n",
    "\n",
    "# Iterate through all folders in the output directory\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "        file_path = os.path.join(folder_path, \"dataset_overview.xlsx\")\n",
    "        if os.path.exists(file_path):  # Check if the file exists\n",
    "            # Read the contents of the sheet 'overall'\n",
    "            df = pd.read_excel(file_path, sheet_name=\"input\")\n",
    "            # Add a new column with the folder name\n",
    "            df[\"subset\"] = folder_name\n",
    "            # Append the DataFrame to the list\n",
    "            all_training_input.append(df)\n",
    "\n",
    "if len(all_training_input) == 0:\n",
    "    print(\"No training datasets found in the output directory.\")\n",
    "    all_training_input = None\n",
    "else:\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    all_training_input = pd.concat(all_training_input, ignore_index=True)\n",
    "    # Split the 'subset' column into three new columns using the regex pattern\n",
    "    all_training_input[[\"polarity\", \"fragmentation_method\", \"collision_energy\"]] = all_training_input[\"combo\"].str.extract(r\"(.*)_(.*)_(.*)\")\n",
    "    # Select only the required columns\n",
    "    columns_to_select = [\"fragmentation_method\", \"polarity\", \"collision_energy\"] + [col for col in all_training_input.columns if \"- \" in col]\n",
    "    all_training_input = all_training_input[columns_to_select]\n",
    "    # Convert the DataFrame to long format\n",
    "    all_training_input = all_training_input.melt(id_vars=[\"fragmentation_method\", \"polarity\", \"collision_energy\"], var_name=\"key\", value_name=\"value\")\n",
    "    all_training_input[[\"reference_library\", \"gt_type\"]] = all_training_input[\"key\"].str.extract(r\"(.*) - (.*)\")\n",
    "    all_training_input[\"gt_type\"] = all_training_input[\"gt_type\"].str.replace(\"gt \", \"\", regex=False)\n",
    "    # Remove all rows with missing values\n",
    "    all_training_input.dropna(inplace=True)\n",
    "    all_training_input = all_training_input[all_training_input[\"value\"] != 0]\n",
    "    # Rename the 'key' column by removing any substring '_neg_' or '_pos_'\n",
    "    all_training_input[\"reference_library\"] = all_training_input[\"reference_library\"].str.replace(r\"_neg_|_pos_\", \"\", regex=True)\n",
    "\n",
    "    # Pivot the table\n",
    "    def _agg(x):\n",
    "        return list(set(x)) if len(set(x)) > 1 else list(set(x))[0]\n",
    "\n",
    "    all_training_input = all_training_input.pivot_table(\n",
    "        index=[\"fragmentation_method\", \"polarity\", \"collision_energy\"], columns=[\"reference_library\", \"gt_type\"], values=\"value\", aggfunc=_agg\n",
    "    ).reset_index()\n",
    "    # Transpose the table\n",
    "    all_training_input = all_training_input.set_index([\"fragmentation_method\", \"polarity\", \"collision_energy\"]).transpose().reset_index()\n",
    "    all_training_input.rename(columns={\"key\": \"reference_library\"}, inplace=True)\n",
    "    # Order the DataFrame by 'reference_library' and 'gt_type'\n",
    "    all_training_input.sort_values(by=[\"reference_library\", \"gt_type\"], inplace=True)\n",
    "    # Group by 'gt_type' and sum all numeric columns\n",
    "    totals = all_training_input.groupby(\"gt_type\").sum(numeric_only=True).reset_index()\n",
    "    # Add a new column 'reference_library' with the value 'total_spectra'\n",
    "    totals[\"reference_library\"] = \"total_spectra\"\n",
    "    # Append the totals to the end of all_training_input\n",
    "    all_training_input = pd.concat([all_training_input, totals], ignore_index=True)\n",
    "\n",
    "\n",
    "# Generate an overview of the validation datasets measured in-house\n",
    "print(f\"\\nGenerating an overview of the validation datasets measured in-house\")\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_validation_results = []\n",
    "\n",
    "# Iterate through all folders in the output directory\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "        file_path = os.path.join(folder_path, \"validation_data.xlsx\")\n",
    "        if os.path.exists(file_path):  # Check if the file exists\n",
    "            # Read the contents of the sheet 'overall'\n",
    "            df = pd.read_excel(file_path, sheet_name=\"overall\")\n",
    "            # Add a new column with the folder name\n",
    "            df[\"subset\"] = folder_name\n",
    "            # Append the DataFrame to the list\n",
    "            all_validation_results.append(df)\n",
    "\n",
    "if len(all_validation_results) == 0:\n",
    "    print(\"No validation datasets found in the output directory.\")\n",
    "    all_validation_results = None\n",
    "else:\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    all_validation_results = pd.concat(all_validation_results, ignore_index=True)\n",
    "    all_validation_results[\"annotated_as\"] = all_validation_results[\"annotated_as_times:relevant\"].map(lambda x: \"relevant\" if x != 0 else \"other\")\n",
    "    all_validation_results.rename(columns={\"row_count\": \"n_features\"}, inplace=True)\n",
    "    all_validation_results[\"percent_features\"] = (100.0 * all_validation_results[\"n_features\"] / all_validation_results.groupby([\"source\", \"subset\"])[\"n_features\"].transform(\"sum\")).round(1)\n",
    "    # Split the 'subset' column into three new columns using the regex pattern\n",
    "    all_validation_results[[\"fragmentation_method\", \"polarity\", \"collision_energy\"]] = all_validation_results[\"subset\"].str.extract(r\".*_(.*)_(.*)_(.*)\")\n",
    "    all_validation_results[\"source\"] = all_validation_results[\"source\"].str.replace(\" - gt \", \" - \", regex=False)\n",
    "    all_validation_results[[\"source\", \"gt_type\"]] = all_validation_results[\"source\"].str.extract(r\"(.*) - (other|relevant)\")\n",
    "    # Order the DataFrame by 'source', 'subset', and 'annotated_as'\n",
    "    all_validation_results.sort_values(by=[\"source\", \"polarity\", \"fragmentation_method\", \"collision_energy\", \"gt_type\", \"annotated_as\"], inplace=True)\n",
    "    # Reorder the columns\n",
    "    all_validation_results = all_validation_results[[\"source\", \"polarity\", \"fragmentation_method\", \"collision_energy\", \"gt_type\", \"annotated_as\", \"n_features\", \"percent_features\"]]\n",
    "\n",
    "\n",
    "# Generate an overview of the inference dataset measured in-house\n",
    "print(f\"\\nGenerating an overview of the inference datasets measured in-house\")\n",
    "# Initialize an empty list to store DataFrames\n",
    "inference_results = []\n",
    "\n",
    "# Iterate through all folders in the output directory\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "        file_path = os.path.join(folder_path, \"inference_data.xlsx\")\n",
    "        if os.path.exists(file_path):  # Check if the file exists\n",
    "            try:\n",
    "                # Read the contents of the sheet 'overall'\n",
    "                df = pd.read_excel(file_path, sheet_name=\"overall\")\n",
    "                # Add a new column with the folder name\n",
    "                df[\"subset\"] = folder_name\n",
    "                # Append the DataFrame to the list\n",
    "                inference_results.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "if len(inference_results) == 0:\n",
    "    print(\"No inference datasets found in the output directory.\")\n",
    "    all_inference_results = None\n",
    "else:\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    all_inference_results = pd.concat(inference_results, ignore_index=True)\n",
    "    all_inference_results[\"annotated_as\"] = all_inference_results[\"annotated_as_times:relevant\"].map(lambda x: \"relevant\" if x != 0 else \"other\")\n",
    "    all_inference_results.drop(columns=[\"annotated_as_times:relevant\"], inplace=True)\n",
    "    all_inference_results = all_inference_results.groupby([\"source\", \"annotated_as\", \"subset\"], as_index=False).agg({\"row_count\": \"sum\"})\n",
    "    all_inference_results[\"subset\"] = all_inference_results[\"subset\"].str.replace(\"(_neg|_pos)$\", \"\\\\1_all\", regex=True)\n",
    "    all_inference_results[[\"fragmentation_method\", \"polarity\", \"collision_energy\"]] = all_inference_results[\"subset\"].str.extract(r\"subset_(.*)_(.*)_(.*)\")\n",
    "    all_inference_results.rename(columns={\"row_count\": \"n_features\"}, inplace=True)\n",
    "    all_inference_results[\"percent_features\"] = (100.0 * all_inference_results[\"n_features\"] / all_inference_results.groupby([\"source\", \"subset\"])[\"n_features\"].transform(\"sum\")).round(1)\n",
    "    all_inference_results = all_inference_results[[\"source\", \"polarity\", \"fragmentation_method\", \"collision_energy\", \"annotated_as\", \"n_features\", \"percent_features\"]]\n",
    "    all_inference_results.sort_values(by=[\"source\", \"polarity\", \"fragmentation_method\", \"collision_energy\", \"annotated_as\"], inplace=True)\n",
    "    all_inference_results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Generate an overview of the training dataset (test only) measured in-house\n",
    "print(f\"\\nGenerating an overview of the training dataset (test only) measured in-house\")\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_test_results = []\n",
    "\n",
    "# Iterate through all folders in the output directory\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "        file_path = os.path.join(folder_path, \"training_data.xlsx\")\n",
    "        if os.path.exists(file_path):  # Check if the file exists\n",
    "            # Read the contents of the sheet 'overall'\n",
    "            df = pd.read_excel(file_path, sheet_name=\"overall\")\n",
    "            # Add a new column with the folder name\n",
    "            df[\"subset\"] = folder_name\n",
    "            # Append the DataFrame to the list\n",
    "            all_test_results.append(df)\n",
    "\n",
    "if len(all_test_results) == 0:\n",
    "    print(\"No training datasets found in the output directory.\")\n",
    "    all_test_results = None\n",
    "else:\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    all_test_results = pd.concat(all_test_results, ignore_index=True)\n",
    "    all_test_results[\"annotated_as\"] = all_test_results[\"annotated_as_times:relevant\"].map(lambda x: \"relevant\" if x != 0 else \"other\")\n",
    "    all_test_results.drop(columns=[\"annotated_as_times:relevant\"], inplace=True)\n",
    "    all_test_results = all_test_results.groupby([\"source\", \"annotated_as\", \"subset\"], as_index=False).agg({\"row_count\": \"sum\"})\n",
    "    all_test_results[\"subset\"] = all_test_results[\"subset\"].str.replace(\"(_neg|_pos)$\", \"\\\\1_all\", regex=True)\n",
    "    all_test_results[[\"fragmentation_method\", \"polarity\", \"collision_energy\"]] = all_test_results[\"subset\"].str.extract(r\"subset_(.*)_(.*)_(.*)\")\n",
    "    all_test_results.rename(columns={\"row_count\": \"n_features\"}, inplace=True)\n",
    "    all_test_results[\"percent_features\"] = (100.0 * all_test_results[\"n_features\"] / all_test_results.groupby([\"source\", \"subset\"])[\"n_features\"].transform(\"sum\")).round(1)\n",
    "    all_test_results[[\"source\", \"gt_type\"]] = all_test_results[\"source\"].str.extract(r\"(.*) - gt (.*)\")\n",
    "    all_test_results = all_test_results[[\"source\", \"polarity\", \"fragmentation_method\", \"collision_energy\", \"gt_type\", \"annotated_as\", \"n_features\", \"percent_features\"]]\n",
    "    all_test_results.sort_values(by=[\"source\", \"polarity\", \"fragmentation_method\", \"collision_energy\", \"gt_type\", \"annotated_as\"], inplace=True)\n",
    "    all_test_results.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Export the two tables to an Excel file\n",
    "output_excel_file = os.path.join(output_dir, \"summary_tables.xlsx\")\n",
    "with pd.ExcelWriter(output_excel_file, engine=\"openpyxl\") as writer:\n",
    "    if all_training_input is not None:\n",
    "        all_training_input.to_excel(writer, sheet_name=\"all_training_input\")\n",
    "    if all_validation_results is not None:\n",
    "        all_validation_results.to_excel(writer, sheet_name=\"all_validation_results\")\n",
    "    if all_inference_results is not None:\n",
    "        all_inference_results.to_excel(writer, sheet_name=\"all_inference_results\")\n",
    "    if all_test_results is not None:\n",
    "        all_test_results.to_excel(writer, sheet_name=\"all_test_results\")\n",
    "\n",
    "print(f\"Exported tables to {output_excel_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
