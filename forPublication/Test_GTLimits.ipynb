{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e6675a",
   "metadata": {},
   "source": [
    "This script shall test the limits of the classifier training. \n",
    "\n",
    "It will use the embeddings calculated from a demo dataset (currently the prenylated flavonoids and chalcones from the MSnLib [public database]), and iteratively remove several **train - relevant** spectra to see how well the **validation** data is predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "df_file = \"../demo/output/PrenylatedCompounds_PublicDBs/df_embeddings.pkl\"\n",
    "\n",
    "training_subsets = {\n",
    "    \"hcd_pos_step[20,45,70]\" : lambda x: (x[\"fragmentation_method\"] == \"hcd\") & (x[\"ionMode\"] == \"positive\") & (x[\"CE\"] in [\"45.0\", \"stepped20,45,70ev(absolute)\"]),\n",
    "}\n",
    "\n",
    "output_dir = \"./GTLimits_output/PrenylatedCompounds_PublicDBs_TrainingSetLimits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09448a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "from AnnoMe.Classification import (\n",
    "    train_and_classify,\n",
    "    generate_embedding_plots,\n",
    "    generate_prediction_overview,\n",
    "    set_random_seeds,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import re\n",
    "\n",
    "import plotnine as p9\n",
    "\n",
    "from natsort import natsorted\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the DataFrame from the specified pickle file\n",
    "df = pd.read_pickle(df_file)\n",
    "\n",
    "# Check if the columns \"ms2deepscore:cleaned_spectra\" and \"ms2deepscore:embeddings\" are present\n",
    "if \"ms2deepscore:cleaned_spectra\" in df.columns and \"ms2deepscore:embeddings\" in df.columns:\n",
    "    # Rename the columns\n",
    "    df.rename(columns={\"ms2deepscore:cleaned_spectra\": \"cleaned_spectra\", \"ms2deepscore:embeddings\": \"embeddings\"}, inplace=True)\n",
    "\n",
    "# show overview of the dataframe column type\n",
    "type_counts = df[\"type\"].value_counts()\n",
    "print(\"DataFrame type counts:\")\n",
    "display(type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e047644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for red_name, red_n in [(\"all\", 1.0), (\"90Percent\", 0.9), (\"80Percent\", 0.8), (\"70Percent\", 0.7), (\"60Percent\", 0.6), (\"50Percent\", 0.5), (\"40Percent\", 0.4), (\"30Percent\", 0.3), (\"20Percent\", 0.2), (\"10Percent\", 0.1)]:\n",
    "    for iteration in range(1, 6):        \n",
    "        # Reduce the DataFrame to the specified percentage of rows\n",
    "        c_df = df.copy()\n",
    "\n",
    "        # Step 1: Exclude rows where type is \"train - relevant\"\n",
    "        c_df_other = c_df[c_df['type'] != \"train - relevant\"]\n",
    "\n",
    "        # Step 2: From the remaining rows, select those where type is \"train_relevant\" and randomly sample red_n fraction of them\n",
    "        c_df_relevant = c_df[c_df['type'] == \"train - relevant\"].sample(frac=red_n, random_state=iteration)\n",
    "\n",
    "        # Step 3: Combine the two DataFrames\n",
    "        df_reduced = pd.concat([c_df_other, c_df_relevant])\n",
    "\n",
    "        num_train_relevant = df_reduced[df_reduced['type'] == \"train - relevant\"].shape[0]\n",
    "        print(f\"Number of rows where type is 'train - relevant': {num_train_relevant}\")\n",
    "\n",
    "        # iterate over the training subsets, produces better output\n",
    "        for subset_name in training_subsets:\n",
    "            print(f\"Processing subset: {subset_name}, with reduction: {red_name}, iteration: {iteration}\")\n",
    "            print(f\"##############################################################################\")\n",
    "            \n",
    "            # Set random seeds for reproducibility\n",
    "            set_random_seeds(42)\n",
    "\n",
    "            # Get the subset function\n",
    "            subset_fn = training_subsets[subset_name]\n",
    "\n",
    "            # Create output directory for the subset\n",
    "            c_output_dir = f\"{output_dir}/subset_{subset_name}_traRel{num_train_relevant}_it{iteration}/\"\n",
    "            os.makedirs(c_output_dir, exist_ok=True)\n",
    "\n",
    "            # subset the dataframe\n",
    "            df_subset = df_reduced[df_reduced.apply(subset_fn, axis=1)].reset_index(drop=True)\n",
    "\n",
    "            # Train\n",
    "            try:\n",
    "                df_train, df_validation, df_inference, df_metrics = train_and_classify(df_subset, subsets=subset_fn, output_dir=c_output_dir)\n",
    "                generate_prediction_overview(df_subset, df_validation, c_output_dir, \"validation\", min_prediction_threshold=13)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training and classification for subset {subset_name}: {e}\")\n",
    "                continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e30c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an overview of the validation datasets measured in-house\n",
    "print(f\"\\nGenerating an overview of the validation datasets measured in-house\")\n",
    "# Initialize an empty list to store DataFrames\n",
    "all_validation_results = []\n",
    "\n",
    "# Iterate through all folders in the output directory\n",
    "for folder_name in os.listdir(output_dir):\n",
    "    folder_path = os.path.join(output_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "        file_path = os.path.join(folder_path, \"validation_data.xlsx\")\n",
    "        if os.path.exists(file_path):  # Check if the file exists\n",
    "            # Read the contents of the sheet 'overall'\n",
    "            df = pd.read_excel(file_path, sheet_name=\"overall\")\n",
    "            # Add a new column with the folder name\n",
    "            df[\"subset\"] = folder_name\n",
    "            # Append the DataFrame to the list\n",
    "            all_validation_results.append(df)\n",
    "\n",
    "if len(all_validation_results) == 0:\n",
    "    print(\"No validation datasets found in the output directory.\")\n",
    "    all_validation_results = None\n",
    "else:\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    all_validation_results = pd.concat(all_validation_results, ignore_index=True)\n",
    "    all_validation_results[\"annotated_as\"] = all_validation_results[\"annotated_as_times:relevant\"].map(lambda x: \"relevant\" if x != 0 else \"other\")\n",
    "    all_validation_results.rename(columns={\"row_count\": \"n_features\"}, inplace=True)\n",
    "    all_validation_results[\"percent_features\"] = (100.0 * all_validation_results[\"n_features\"] / all_validation_results.groupby([\"source\", \"subset\"])[\"n_features\"].transform(\"sum\")).round(1)\n",
    "    # Split the 'subset' column into three new columns using the regex pattern\n",
    "    all_validation_results[[\"fragmentation_method\", \"polarity\", \"collision_energy\", \"reduction\", \"iteration\"]] = all_validation_results[\"subset\"].str.extract(r\"subset_(.*)_(.*)_(.*)_(.*)_(.*)\")\n",
    "    all_validation_results[\"source\"] = all_validation_results[\"source\"].str.replace(\" - gt \", \" - \", regex=False)\n",
    "    all_validation_results[[\"source\", \"gt_type\"]] = all_validation_results[\"source\"].str.extract(r\"(.*) - (other|relevant)\")\n",
    "    # Order the DataFrame by 'source', 'subset', and 'annotated_as'\n",
    "    all_validation_results.sort_values(by=[\"source\", \"polarity\", \"fragmentation_method\", \"collision_energy\", \"reduction\", \"iteration\", \"gt_type\", \"annotated_as\"], inplace=True)\n",
    "    # Reorder the columns\n",
    "    all_validation_results = all_validation_results[[\"source\", \"polarity\", \"fragmentation_method\", \"collision_energy\", \"reduction\", \"iteration\", \"gt_type\", \"annotated_as\", \"n_features\", \"percent_features\"]]\n",
    "\n",
    "\n",
    "display(all_validation_results.head())\n",
    "# Ensure the 'reduction' column is sorted in natural order\n",
    "\n",
    "all_validation_results[\"reduction\"] = pd.Categorical(\n",
    "    all_validation_results[\"reduction\"],\n",
    "    categories=natsorted(all_validation_results[\"reduction\"].unique()),\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "p = p9.ggplot(all_validation_results, p9.aes(x=\"reduction\", y=\"percent_features\", colour=\"gt_type + ': ' + annotated_as\")) + \\\n",
    "    p9.theme_bw() + \\\n",
    "    p9.geom_boxplot() + \\\n",
    "    p9.geom_jitter(size=1, alpha=0.5, height = 0, width = 0.1) + \\\n",
    "    p9.facet_grid(\"source ~ polarity + fragmentation_method + collision_energy\") + \\\n",
    "    p9.theme(axis_text_x=p9.element_text(rotation=90, hjust=1)) + \\\n",
    "    p9.labs(title=\"Overview of validation datasets measured in-house\", x=\"Reduction\", y=\"Percent of features\", colour=\"GT Type\") + \\\n",
    "    p9.scale_color_manual(values={\"other: other\": \"#41e541\", \"other: relevant\": \"#2999e9\", \"relevant: other\": \"#1f77b4\", \"relevant: relevant\": \"#2ca02c\"})\n",
    "display(p)\n",
    "# Save the plot to a file\n",
    "output_plot_file = os.path.join(output_dir, \"validation_overview.png\")\n",
    "p.save(output_plot_file, width=12, height=8, dpi=300)\n",
    "\n",
    "# Export the two tables to an Excel file\n",
    "output_excel_file = os.path.join(output_dir, \"summary_tables.xlsx\")\n",
    "with pd.ExcelWriter(output_excel_file, engine=\"openpyxl\") as writer:\n",
    "    if all_validation_results is not None:\n",
    "        all_validation_results.to_excel(writer, sheet_name=\"all_validation_results\")\n",
    "\n",
    "print(f\"Exported tables to {output_excel_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
